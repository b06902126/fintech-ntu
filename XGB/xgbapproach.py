# -*- coding: utf-8 -*-
"""XgbApproach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15gSQ7UMVyZlsuNPn8RU9fDt3wwmHtsih

# Access to files
"""

!cat /proc/meminfo

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import os
# your workspace in your drive
workspace = 'HW_FINPROJ'

os.chdir(os.path.join('/content/gdrive/My Drive/', workspace))

# !unzip data.zip

# !unzip txn_2019H1.zip
# !unzip txn_2019H2.zip
# !unzip txn_2020H1.zip
# !unzip txn_2020H2.zip
# !unzip stock_info.zip

"""# Loading data & Preprocessing"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

# 2019
txn191 = pd.read_csv('txn_2019Q1.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn192 = pd.read_csv('txn_2019Q2.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn193 = pd.read_csv('txn_2019Q3.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn194 = pd.read_csv('txn_2019Q4.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])

stock191 = pd.read_csv('stock_info_2019Q1.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock192 = pd.read_csv('stock_info_2019Q2.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock193 = pd.read_csv('stock_info_2019Q3.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock194 = pd.read_csv('stock_info_2019Q4.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])

ts191 = pd.merge(txn191, stock191, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts192 = pd.merge(txn192, stock192, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts193 = pd.merge(txn193, stock193, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts194 = pd.merge(txn194, stock194, on=['DATE_RANK', 'STOCK_NO'], how='left')

# 2020
txn201 = pd.read_csv('txn_2020Q1.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn202 = pd.read_csv('txn_2020Q2.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn203 = pd.read_csv('txn_2020Q3.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
txn204 = pd.read_csv('txn_2020Q4.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])

stock201 = pd.read_csv('stock_info_2020Q1.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock202 = pd.read_csv('stock_info_2020Q2.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock203 = pd.read_csv('stock_info_2020Q3.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
stock204 = pd.read_csv('stock_info_2020Q4.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])

ts201 = pd.merge(txn201, stock201, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts202 = pd.merge(txn202, stock202, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts203 = pd.merge(txn203, stock203, on=['DATE_RANK', 'STOCK_NO'], how='left')
ts204 = pd.merge(txn204, stock204, on=['DATE_RANK', 'STOCK_NO'], how='left')

# 2021
txn211 = pd.read_csv('txn_2021Q1.csv', names=["DATE_RANK","CUST_NO","BS_CODE","MARKET_CODE","STOCK_NO","COMMISION_TYPE_CODE","PRICE","STOCKS","ROI"])
stock211 = pd.read_csv('stock_info_2021Q1.csv', names=['DATE_RANK','STOCK_NO','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D'])
ts211 = pd.merge(txn211, stock211, on=['DATE_RANK', 'STOCK_NO'], how='left')

# merge
ts = pd.concat([ts191, ts192, ts193, ts194, ts201, ts202, ts203, ts204, ts211])
cust = pd.read_csv('cust_info_1.csv', names=["CUST_NO","AGE_LEVEL",'OPEN_ACCT_YEAR','SOURCE_CODE','BREACH_IND','BREACH_DATE_RANK','BREACH_RANK','INVESTMENT_TXN_CODE','BUY_COUNT','SELL_COUNT',"NONTXN_COUNT"], skiprows=[0])
cust = cust.drop_duplicates(subset=['CUST_NO'], keep='first')
# cust.drop(['BREACH_DATE_RANK','BREACH_RANK'], axis=1, inplace=True)
print(cust.shape)
df = pd.merge(ts, cust, on=['CUST_NO'], how='left')

del txn191, txn192, txn193, txn194, txn201, txn202, txn203, txn204, txn211
del stock191, stock192, stock193, stock194, stock201, stock202, stock203, stock204, stock211
del ts191, ts192, ts193, ts194, ts201, ts202, ts203, ts204, ts211
del ts, cust

df #shape should be (n, 30)

df['BREACH_IND_NEW'] = np.where(df['BREACH_DATE_RANK'] == df['DATE_RANK'], 1, 0)

# deleting missing data
df.drop(['BREACH_IND','BREACH_DATE_RANK','BREACH_RANK'], axis=1, inplace=True)

df.dropna(inplace=True)
df.BREACH_IND_NEW.value_counts()

print("Breach ratio after deleting missing: {}%".format((df.BREACH_IND_NEW.value_counts()[1]/(df.BREACH_IND_NEW.value_counts()[0]+df.BREACH_IND_NEW.value_counts()[1]))*100))

df_den = df.copy() # for density plot

# drop useless variables
df.drop(["DATE_RANK","CUST_NO","STOCK_NO"], axis=1, inplace=True)

# reencoding some catgorical variables
mapper_bs = {"B":1,"S":0}
mapper_sc = {"A":0,"B":1}
mapper_ct = {"H":3,"M":2,"S":1}
mapper_ctc = {'0':'x', '1':'z', '2':'c', '5':'jc', 'A':'a'}
df["BS_CODE"] = df["BS_CODE"].map(mapper_bs)
df["SOURCE_CODE"] = df["SOURCE_CODE"].map(mapper_sc)
df["CAPITAL_TYPE"] = df["CAPITAL_TYPE"].map(mapper_ct)
df['COMMISION_TYPE_CODE'] = df['COMMISION_TYPE_CODE'].map(mapper_ctc)

df = pd.get_dummies(df)
df.dtypes

"""# Descriptive Statistics and Observations"""

import seaborn as sns

"""### Density Plots and Bar Plots"""

df_den.dtypes

mapper_brn = {0:'Not Breach', 1:'Breach'}
df_den["IS_BREACH"] = df_den["BREACH_IND_NEW"].map(mapper_brn)

Conti = ['OPEN_ACCT_YEAR', 'INVESTMENT_TXN_CODE', 'AMONT', 'PRICE']

n = 2
m = 2
fig, ax = plt.subplots(n,m, figsize=(10,10))

for i in range(n):
  for j in range(m):
    var = Conti[2*i+j]
    subset = df_den[[var, "IS_BREACH"]]
    
    sns.kdeplot(subset[subset.IS_BREACH == "Not Breach"][var], bw_adjust=5, ax=ax[i][j], label = 'Not Breach')
    sns.kdeplot(subset[subset.IS_BREACH == "Breach"][var], ax=ax[i][j], label = 'Breach')
    ax[i][j].legend()

Descrete = ['AGE_LEVEL', 'COMMISION_TYPE_CODE', 'SOURCE_CODE', 'CAPITAL_TYPE']

n = 2
m = 2
fig, ax = plt.subplots(n,m, figsize=(10,10))


for i in range(n):
  for j in range(m):

    x, y, hue = Descrete[2*i+j], "proportion", "IS_BREACH"
    hue_order = ["Not Breach", "Breach"]

    (df_den[x]
    .groupby(df_den[hue])
    .value_counts(normalize=True)
    .rename(y)
    .reset_index()
    .pipe((sns.barplot, "data"), x=x, y=y, hue=hue, hue_order=hue_order, ax=ax[i][j])
    )
    ax[i][j].legend()

"""### Correlation"""

sns.set_theme(style="white")

# Compute the correlation matrix
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5})

"""### Boxplot"""

df_norm = (df - df.mean())/df.std(ddof=0)
df_norm

Conti_norm = ['PRICE','STOCKS','ROI','OPEN_PRICE','MAX_PRICE','MIN_PRICE','CLOSE_PRICE','VOLUME','AMONT','CAPITAL_TYPE','ALPHA','BETA_21D','BETA_65D','BETA_250D','AGE_LEVEL','OPEN_ACCT_YEAR','INVESTMENT_TXN_CODE','BUY_COUNT','SELL_COUNT',"NONTXN_COUNT"]
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 11))
plt.xlim(-3, 3)
sns.boxplot(data=df_norm[Conti_norm], orient='h', flierprops = dict(marker = 'o', markeredgecolor='navy', alpha=.01, ms=10, markersize = 0.2))
plt.show()

del df_norm

"""# Define X and Y"""

# use all data
data = df

# use partial data
breachSubset = df[df["BREACH_IND_NEW"] == 1]
nonBreachSubset = df[df["BREACH_IND_NEW"] == 0]

data = pd.concat([nonBreachSubset.sample(n=breachSubset.shape[0]*5000, random_state=2021), breachSubset])
data

dtaY = data.BREACH_IND_NEW
dtaX = data.drop(["BREACH_IND_NEW"], axis=1)

"""# Split Data and Training"""

from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import r2_score, auc, mean_squared_error, accuracy_score, precision_recall_curve, roc_curve, recall_score, precision_score, f1_score

import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from xgboost import plot_importance, plot_tree

X_train, X_test, y_train, y_test = train_test_split(dtaX, dtaY, test_size=0.2, random_state=50, stratify=dtaY)

print(X_train.shape, X_test.shape)

params = {
    'max_depth': 20,
    'n_estimators': 1000,
    'subsample': 1,
    'colsample_bytree': 0.7, 
    'learning_rate': 0.01, 
    'reg_alpha': 0.1,
    'reg_lambda': 1e-05,
    'min_child_weight': 23,
    'gamma': 0.2
}

clf = xgb.XGBClassifier(**params, scale_pos_weight=5000)

model = clf.fit(X_train, y_train, verbose=True, eval_set=[(X_test, y_test)], eval_metric=["aucpr"])

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Params :", params)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

"""# Metrics

### Predict TestingSet
"""

# get probability
probs_01 = model.predict_proba(X_test)
probs = probs_01[:,1]

"""### ROC-AUC and optimal threshold"""

print("Validating...")

# get false postive rate, true positive rate and threshold
fpr, tpr, threshold = roc_curve(y_test, probs)

# 2 types of optimal threshold
g_idx = np.argmax(np.sqrt(tpr * (1-fpr)))
h_idx = np.argmax(tpr - fpr)
idx = [g_idx, h_idx]

# select type of threshold
typeidx = 1

# some metrics
best_threshold = threshold[idx[typeidx]]
print("best threshold is:", best_threshold)

check2 = [1 if i > best_threshold else 0 for i in probs]

score = precision_score(y_test, check2)
print('precision score: {:.6f}'.format(score))

score = recall_score(y_test, check2)
print('recall score: {:.6f}'.format(score))

score = f1_score(y_test, check2)
print('F1 score: {:.6f}'.format(score))

# ROC_AUC
roc_auc = auc(fpr, tpr)
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label='Baseline')
plt.scatter(fpr[idx[typeidx]], tpr[idx[typeidx]], marker='o', color='black', label='Best')
plt.xlim([-0.02, 1.0])
plt.ylim([0.0, 1.05])

# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

# calculate pr-curve
precision, recall, thresholds = precision_recall_curve(y_test, probs)
prc_auc = auc(recall, precision)
# 2 types of optimal threshold
fscore = (2 * precision * recall) / (precision + recall)
idx = np.argmax(fscore)

# some metrics
best_threshold = thresholds[idx]
print("best threshold is:", best_threshold)

check2 = [1 if i > best_threshold else 0 for i in probs]

score = precision_score(y_test, check2)
print('precision score: {:.6f}'.format(score))

score = recall_score(y_test, check2)
print('recall score: {:.6f}'.format(score))

score = f1_score(y_test, check2)
print('F1 score: {:.6f}'.format(score))

# plot the roc curve for the model
plt.plot(recall, precision, color='darkorange', lw=lw, label='PRC curve (area = %0.2f)' % prc_auc)
# axis labels
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
# show the plot
plt.show()

# plot feature importance
plt.rcParams["figure.figsize"] = (7, 7)
plot_importance(model, max_num_features=15)
plt.show()

plt.rcParams["figure.figsize"] = (170, 140)
plot_tree(model)
plt.savefig('books_read.png')

import scikitplot as skplt
skplt.metrics.plot_cumulative_gain(y_test, probs_01)
plt.ylabel('Cumulative recall')
plt.title('Lift Chart')

!pip install scikit-plot